# Assistant RAG avec Mistral

Ce projet implÃ©mente un assistant virtuel basÃ© sur le modÃ¨le Mistral, utilisant la technique de Retrieval-Augmented Generation (RAG) pour fournir des rÃ©ponses prÃ©cises et contextuelles Ã  partir d'une base de connaissances personnalisÃ©e.
L'objectif est de reprendre un prototype rÃ©alisÃ© qui Ã©tait fonctionnel et de procÃ©der Ã  quelques amÃ©liorations afin d'obtenir des meilleurs rÃ©sultats.
Les amÃ©liorations seront visibles avec une comparaison des mÃ©triques ragas sur le prototype vs la nouvelle structuration du projet.

## FonctionnalitÃ©s

- ğŸ—„ï¸ **Recherche sÃ©mantique** avec FAISS pour trouver les documents pertinents (PDF Ã  disposition)
- ğŸ—„ï¸ **Recherche dans une base relationnelle** avec une base de donnÃ©es PostreSQL pour effectuer une recherche des Ã©lÃ©ments chiffrÃ©s.
- ğŸ” **Choix du systÃ¨me** pour sÃ©lectionner le bon type de donnÃ©e Ã  prendre.
- ğŸ¤– **GÃ©nÃ©ration de rÃ©ponses** avec les modÃ¨les Mistral (Small ou Large)
- âš™ï¸ **ParamÃ¨tres personnalisables** (modÃ¨le, nombre de documents, score minimum)

## PrÃ©requis

- Python 3.9+ 
- ClÃ© API Mistral (obtenue sur [console.mistral.ai](https://console.mistral.ai/))
- Avoir une solution de stockage en local (PostreSQL utilisÃ© ici)

## Installation

1. **Cloner le dÃ©pÃ´t**

```
git clone git@github.com:SCFlorian/Evaluation_LLM.git
cd Evaluation_LLM
```

2. **Installez les dÃ©pendances : Le projet utilise pyproject.toml pour la gestion des dÃ©pendances :**
```
poetry install --no-root
```
3. **Ouvrir le projet dans VS Code :**
```
code .
```
4. **Configurez lâ€™environnement Python dans VS Code**
	1.	Installez lâ€™extension Python (si ce nâ€™est pas dÃ©jÃ  fait).
	2.	Appuyez sur Ctrl+Shift+P (Windows/Linux) ou Cmd+Shift+P (Mac).
	4.	Recherchez â€œPython: Select Interpreterâ€.
	5.	SÃ©lectionnez lâ€™environnement crÃ©Ã© par Poetry ou celui dans lequel tu as installÃ© le projet.

5. **Configurer la clÃ© API**

CrÃ©ez un fichier `.env` Ã  la racine du projet avec le contenu suivant :

```
MISTRAL_API_KEY=votre_clÃ©_api_mistral
DATABASE_URL="postgresql://**user**:**mdp**e@localhost:5432/**nom_bdd**"
```

## Structure du projet

```
.
â”œâ”€â”€ data/                                      # Dossier contenant nos fichiers csv d'Ã©valuation
â”‚   â””â”€â”€ processed/  
â”‚       â”œâ”€â”€first_ragas_results.csv             # RÃ©sultats de la premiÃ¨re Ã©valuation ragas
â”‚       â”œâ”€â”€resultat_evaluation.csv             # GÃ©nÃ©ration des questions/rÃ©ponses
â”‚   â””â”€â”€ raw/                                   # Scripts de gÃ©nÃ©ration des Ã©valuations
â”‚       â”œâ”€â”€Reddit 1.pdf                        # Premier fichier Reddit
â”‚       â”œâ”€â”€Reddit 2.pdf                        # DeuxiÃ¨me fichier Reddit
â”‚       â”œâ”€â”€Reddit 3.pdf                        # TroisiÃ¨me fichier Reddit
â”‚       â”œâ”€â”€Reddit 4.pdf                        # QuatriÃ¨me fichier Reddit
â”‚       â”œâ”€â”€regular NBA.xlsx                    # Fichier excel avec les statistiques par joueur
â”œâ”€â”€ database/                                  # CrÃ©ation et gÃ©nÃ©ration de la BDD
â”‚   â”œâ”€â”€creation_db.py                          # Script avec les classes de nos tables
â”‚   â”œâ”€â”€generation_db.py                        # GÃ©nÃ©ration de notre BDD et ajout du fichier excel
â”‚   â”œâ”€â”€sql_tool.py                             # PrÃ©paration de la chaÃ®ne pour rÃ©cupÃ©rer les informations depuis la BDD
â”œâ”€â”€ evaluations/                               # Scripts de gÃ©nÃ©ration des Ã©valuations
â”‚   â”œâ”€â”€first_ragas_evaluation.py               # Script de la premiÃ¨re Ã©valuation ragas
â”‚   â”œâ”€â”€generation_answers.py                   # Script de la gÃ©nÃ©ration des questions/rÃ©ponses
â”œâ”€â”€ notebooks/                                 # Dossier contenant les notebooks pour une meilleure comprÃ©hension des donnÃ©es
â”‚   â”œâ”€â”€notebook_analyse_exploratoire.ipynb     # Notebook sur la prÃ©paration du fichier excel pour les Ã©valuations
â”œâ”€â”€ rag/                                       # Scripts contenant les fonctions du projet
â”‚   â”œâ”€â”€cleaning_excel.py                       # Script prÃ©parant les fichiers excel (dont nettoyage) pour la BDD
â”‚   â”œâ”€â”€config.py                               # Script contenant les configurations (le nom des paramÃ¨tres, des modÃ¨les etc)
â”‚   â”œâ”€â”€creation_llm.py                         # Script contenant la crÃ©ation du LLM (initialisation du modÃ¨le, gÃ©nÃ©ration de la rÃ©ponse)
â”‚   â”œâ”€â”€data_loader.py                          # Script contenant le chargement des documents
â”‚   â”œâ”€â”€retrieval.py                            # Script contenant la recherche dans la documentation
â”‚   â”œâ”€â”€schema_validation.py                    # Script contenant les schÃ©mas de validation Pydantic
â”‚   â”œâ”€â”€vector_store.py                         # Script contenant les diffÃ©rentes fonctions allant de la crÃ©ation des dÃ©coupages Ã  l'enregistrement des vecteurs
â”œâ”€â”€ scripts/                                   # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€build_index.py                          # Les documents dÃ©coupÃ©s en format pkl
â”‚   â”œâ”€â”€chat.py                                 # la base d'index FAISS
â”‚   â”œâ”€â”€generation_db.py                        # la base d'index FAISS
â”œâ”€â”€ tests/                                     # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€valisation_pydantic.py                  # Les documents dÃ©coupÃ©s en format pkl
â”œâ”€â”€ vector_db/                                 # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€document_chunks.pkl                     # Les documents dÃ©coupÃ©s en format pkl
â”‚   â”œâ”€â”€faiss_index.idx                         # la base d'index FAISS
â”œâ”€â”€ .env                                       # Enregistrement des informations qui ne doivent pas Ãªtre publiÃ©es
â”œâ”€â”€ .gitignore                                 # Permet de ne pas afficher les Ã©lÃ©ments sÃ©lectionnÃ©s sur GitHub
â”œâ”€â”€ app.py                                     # Orchestre la vectorisation et la sauvegarde
â”œâ”€â”€ MistralChat.py                             # Script pour le lancement de l'API et de l'interface avec Streamlit
â”œâ”€â”€ poetry.lock                                # Pas versionnÃ© sur Git
â”œâ”€â”€ pyproject.toml                             # Gestion des dÃ©pendances Poetry
â”œâ”€â”€ README.md                                  # Documentation du projet

```
## Utilisation

### 1. Ajouter des documents

Placez vos documents dans le dossier `data/raw`.
Deux formats sont suportÃ©s pour le projet, il est possible de placer des documents en PDF ainsi que des fichiers excel.
- Les documents en PDF seront transformÃ©s et enregistrÃ©s dans une base vectorielle.
- Les fichiers excel seront nettoyÃ©s et ajoutÃ©s dans une base de donnÃ©es relationnelle (PostreSQL utilisÃ© ici).
- Pour maintenir une cohÃ©rence et une fiabilitÃ© dans nos donnÃ©es, les fichiers excel doivent respecter un certain format (vous pouvez par exemple celui utilisÃ©  dans data/raw).


### 2. Enregistrement des documents
#### Indexer les documents (PDF)

ExÃ©cutez le script d'indexation pour traiter les documents et crÃ©er l'index FAISS :

```bash
python build_index.py
```
Le fichier va s'appuyer sur les fonctions se trouvant dans **vector_store** & **embeddings**
Ce script va :
1. Charger les documents depuis le dossier `data/raw` avec le script data_loader.
2. DÃ©couper les documents en chunks en appelant le script embeddings.
Une fois le texte extrait (en mÃ©moire aprÃ¨s lancement de lâ€™indexer,) il est trop long pour Ãªtre envoyÃ© tel quel Ã  un LLM. Il faut le dÃ©couper en morceaux digeste pour le modÃ¨le.

Utilisation de `Langchain` avec `RecursiveCharacterTextSplitter`

**La stratÃ©gie utilisÃ©e ici :**

- **`CHUNK_SIZE = 1500`**Â : Chaque morceau fait environ 1500 caractÃ¨res (environ 300-400 mots).
- **`CHUNK_OVERLAP = 150`**Â : Il y a un chevauchement de 150 caractÃ¨res entre deux morceaux consÃ©cutifs.

Â Cela permet dâ€™Ã©viter de couper une phrase importante en plein milieu. Si une phrase est coupÃ©e, la fin se retrouvera au dÃ©but du morceau suivant grÃ¢ce Ã  l'overlap.
3. GÃ©nÃ©rer des embeddings avec Mistral
**Script :**Â `rag/embeddings.py`
C'est l'Ã©tape de traduction. L'ordinateur ne comprend pas le texte, il comprend les chiffres.
- **Outil :**Â API Mistral (`mistral-embed`).
- **Action :**Â Chaque dÃ©coupage de texte est envoyÃ© Ã  Mistral, qui renvoie une liste de nombres (un vecteur) reprÃ©sentant leÂ **sens**Â du texte.
4. CrÃ©er un index FAISS pour la recherche sÃ©mantique
**Script :**Â `rag/vector_store.py`
- **Outil :**Â `FAISS`Â (Facebook AI Similarity Search).
- **Action :**Â Tous ces vecteurs sont stockÃ©s dans un fichierÂ `vector_db/faiss_index.idx`. C'est une base de donnÃ©es ultra-rapide optimisÃ©e pour trouver les vecteurs "voisins".
- **MÃ©tadonnÃ©es :**Â En plus du vecteur, le script stocke le lien vers le fichier source (`filename: "Reddit 1.pdf"`,Â `page: 2`).
- **`IndexFlatIP`**Â : Produit scalaire (cosine similarity aprÃ¨s normalisation). Il fournit des rÃ©sultats de recherche de voisins les plus proches exacts, ce qui le rend adaptÃ© aux applications oÃ¹ la prÃ©cision est essentielle. leÂ **produit scalaire**Â sert tout simplement Ã  mesurer Ã  quel point deux vecteurs se ressemblent.
Pourquoi on utilise Ã§a (au lieu de la distance) ? Le produit scalaire mesure lâ€™**alignement.** Et dans les embeddings modernes (texte, images, IA) : des choses similaires pointent dans la mÃªme direction dans lâ€™espace.
5. Sauvegarder l'index et les chunks dans le dossier `vector_db/`

#### Enregistrements des Ã©lÃ©ments chiffrÃ©s

L'enregistrement des datas dans la base donnÃ©es se fait dans une base PostreSQL en local.
1. Connexion Ã  une base PostreSQL
Choix de la BDD PostreSQL pour sa simplicitÃ© avec l'ORM SQLAlchemy.
CrÃ©ation d'une bDD en local :
a. Ouvrez votre terminal puis lancez les commandes une Ã  une :
```
psql
CREATE DATABASE sportsee_nba_stats;
CREATE USER sportsee_user WITH PASSWORD '***';
GRANT ALL PRIVILEGES ON DATABASE sportsee_nba_stats TO sportsee_user;
ALTER DATABASE sportsee_nba_stats OWNER TO sportsee_user;
```
b. AccÃ¨s Ã  la BDD
```
psql -U sportsee_user -d sportsee_nba_stats
```
2. Initialisation de la base avec `database/creation_db`

Ce script va nous permettre d'initialiser notre base afin qu'elle soit accessible. On utilise PostreSQL avc l'ORM SQLAlchemy pour faire le lien netre Python et PostrezSQL.
On va y crÃ©er deux tables :
- la table player : retrace les informations des joueurs (nom, Ã¢ge, Ã©quipe)
- la table stats : retrace les performances statistiques de chaque joueur.
Exemple pour la table player :
```
class Player(Base):
    """ PrÃ©paration d'une table avec les informations clÃ©s des joueurs (nom, Ã¢ge) et leur Ã©quipe."""
    __tablename__ = "player"

    id = Column(Integer, primary_key=True)
    name = Column(String,nullable=False, unique=True)
    age = Column(Integer)
    acronym_team = Column(String)
    team = Column(String)

    stats = relationship("Stats", back_populates="player_relation")
```
3. Nettoyage des fichiers excel
Avant de pouvoir rÃ©cupÃ©rer les informations dans les tables, il y a quelques ajustements Ã  rÃ©aliser sur le fichier excel.
- On doit procÃ©der Ã  du nettoyage sur le dataframe des statistiques des joueurs comme la suppression de colonnes vides ou encore le renommage de certaines colonnes.
- On doit crÃ©er un fichier player car il n'existe pas en tant que tel dans les informations donnÃ©es par l'entreprise.
- On prÃ©pare un fichier avec la dÃ©finition des acronymes du noms des varibales statistiques pour qu'elles soient visibles dans les mÃ©tadonnÃ©es.

4. Lancement de `generation_db.py`
Ce script va faire le lien entre le script creation_db et va permettre d'envoyer les donnÃ©es dans la BDD.
- A l'interieur il est indiquÃ© de prendre chaque ligne des nouveaux fichiers excel et de les ajouter.
Exemple pour la table player :
```
# =====================================================
# Enregistrement du nom des Ã©quipes NBA dans une table
# =====================================================
session_player = SessionLocal()
for _, row in df_player.iterrows():
    team = Player(
        name=row["Player"],
        age =row["Age"],
        acronym_team = row["Team"],
        team = row["Team_full_name"]
    )
    session_player.add(team)

session_player.commit()
session_player.close()
```
- Les tables sont dÃ©sormais Ã  jour. Si vous avez installÃ© pgAdmin, vous pouvz siualsier facilement l'intÃ©gration des donnÃ©es.

