# Assistant RAG avec Llama

Ce projet implÃ©mente un assistant virtuel basÃ© sur un modÃ¨le Llama, utilisant la technique de Retrieval-Augmented Generation (RAG) pour fournir des rÃ©ponses prÃ©cises et contextuelles Ã  partir d'une base de connaissances personnalisÃ©e.
L'objectif est de reprendre un prototype rÃ©alisÃ© qui Ã©tait fonctionnel et de procÃ©der Ã  quelques amÃ©liorations afin d'obtenir des meilleurs rÃ©sultats.
Les amÃ©liorations seront visibles avec une comparaison des mÃ©triques ragas sur le prototype vs la nouvelle structuration du projet.

## FonctionnalitÃ©s

- ğŸ—„ï¸ **CrÃ©ation des vecteurs** avec HuggingFaceEmbeddings.
- ğŸ—„ï¸ **Recherche sÃ©mantique** avec FAISS pour trouver les documents pertinents (PDF Ã  disposition).
- ğŸ—„ï¸ **Recherche dans une base relationnelle** avec une base de donnÃ©es PostreSQL pour effectuer une recherche des Ã©lÃ©ments chiffrÃ©s.
- ğŸ” **Choix du systÃ¨me** pour sÃ©lectionner le bon type de donnÃ©e Ã  prendre.
- ğŸ¤– **GÃ©nÃ©ration de rÃ©ponses** avec un modÃ¨le Llama (llama-3.3-70b-versatile) via Groq.
- âš™ï¸ **ParamÃ¨tres personnalisables** (modÃ¨le, nombre de documents, score minimum, etc).

## PrÃ©requis

- Python 3.9+ 
- ClÃ© API Groq (avoir un compte et se diriger vers : https://console.groq.com/keys)
- Avoir une solution de stockage en local (PostreSQL utilisÃ© ici)

## Installation

1. **Cloner le dÃ©pÃ´t**

```
git clone git@github.com:SCFlorian/Evaluation_LLM.git
cd Evaluation_LLM
```

2. **Installez les dÃ©pendances : Le projet utilise pyproject.toml pour la gestion des dÃ©pendances :**
```
poetry install --no-root
```
3. **Ouvrir le projet dans VS Code :**
```
code .
```
4. **Configurez lâ€™environnement Python dans VS Code**
	1.	Installez lâ€™extension Python (si ce nâ€™est pas dÃ©jÃ  fait).
	2.	Appuyez sur Ctrl+Shift+P (Windows/Linux) ou Cmd+Shift+P (Mac).
	4.	Recherchez â€œPython: Select Interpreterâ€.
	5.	SÃ©lectionnez lâ€™environnement crÃ©Ã© par Poetry ou celui dans lequel tu as installÃ© le projet.

5. **Configurer la clÃ© API**

CrÃ©ez un fichier `.env` Ã  la racine du projet avec le contenu suivant :

```
GROQ_API_KEY=votre_clÃ©_api_groq
DATABASE_URL="postgresql://**user**:**mdp**e@localhost:5432/**nom_bdd**"
```

## Structure du projet

```
.
â”œâ”€â”€ data/                                      # Dossier contenant nos fichiers csv d'Ã©valuation
â”‚   â””â”€â”€ processed/  
â”‚       â”œâ”€â”€first_ragas_results.csv             # RÃ©sultats de la premiÃ¨re Ã©valuation ragas
â”‚       â”œâ”€â”€resultat_evaluation.csv             # GÃ©nÃ©ration des questions/rÃ©ponses
â”‚   â””â”€â”€ raw/                                   # Scripts de gÃ©nÃ©ration des Ã©valuations
â”‚       â”œâ”€â”€Reddit 1.pdf                        # Premier fichier Reddit
â”‚       â”œâ”€â”€Reddit 2.pdf                        # DeuxiÃ¨me fichier Reddit
â”‚       â”œâ”€â”€Reddit 3.pdf                        # TroisiÃ¨me fichier Reddit
â”‚       â”œâ”€â”€Reddit 4.pdf                        # QuatriÃ¨me fichier Reddit
â”‚       â”œâ”€â”€regular NBA.xlsx                    # Fichier excel avec les statistiques par joueur
â”œâ”€â”€ database/                                  # CrÃ©ation et gÃ©nÃ©ration de la BDD
â”‚   â”œâ”€â”€creation_db.py                          # Script avec les classes de nos tables
â”‚   â”œâ”€â”€generation_db.py                        # GÃ©nÃ©ration de notre BDD et ajout du fichier excel
â”‚   â”œâ”€â”€sql_tool.py                             # PrÃ©paration de la chaÃ®ne pour rÃ©cupÃ©rer les informations depuis la BDD
â”œâ”€â”€ evaluations/                               # Scripts de gÃ©nÃ©ration des Ã©valuations
â”‚   â”œâ”€â”€first_ragas_evaluation.py               # Script de la premiÃ¨re Ã©valuation ragas
â”‚   â”œâ”€â”€generation_answers.py                   # Script de la gÃ©nÃ©ration des questions/rÃ©ponses
â”œâ”€â”€ notebooks/                                 # Dossier contenant les notebooks pour une meilleure comprÃ©hension des donnÃ©es
â”‚   â”œâ”€â”€notebook_analyse_exploratoire.ipynb     # Notebook sur la prÃ©paration du fichier excel pour les Ã©valuations
â”œâ”€â”€ rag/                                       # Scripts contenant les fonctions du projet
â”‚   â”œâ”€â”€cleaning_excel.py                       # Script prÃ©parant les fichiers excel (dont nettoyage) pour la BDD
â”‚   â”œâ”€â”€config.py                               # Script contenant les configurations (le nom des paramÃ¨tres, des modÃ¨les etc)
â”‚   â”œâ”€â”€creation_llm.py                         # Script contenant la crÃ©ation du LLM (initialisation du modÃ¨le, gÃ©nÃ©ration de la rÃ©ponse)
â”‚   â”œâ”€â”€data_loader.py                          # Script contenant le chargement des documents
â”‚   â”œâ”€â”€retrieval.py                            # Script contenant la recherche dans la documentation
â”‚   â”œâ”€â”€schema_validation.py                    # Script contenant les schÃ©mas de validation Pydantic
â”‚   â”œâ”€â”€vector_store.py                         # Script contenant les diffÃ©rentes fonctions allant de la crÃ©ation des dÃ©coupages Ã  l'enregistrement des vecteurs
â”œâ”€â”€ scripts/                                   # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€build_index.py                          # Les documents dÃ©coupÃ©s en format pkl
â”‚   â”œâ”€â”€chat.py                                 # la base d'index FAISS
â”‚   â”œâ”€â”€generation_db.py                        # la base d'index FAISS
â”œâ”€â”€ tests/                                     # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€valisation_pydantic.py                  # Les documents dÃ©coupÃ©s en format pkl
â”œâ”€â”€ vector_db/                                 # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€document_chunks.pkl                     # Les documents dÃ©coupÃ©s en format pkl
â”‚   â”œâ”€â”€faiss_index.idx                         # la base d'index FAISS
â”œâ”€â”€ .env                                       # Enregistrement des informations qui ne doivent pas Ãªtre publiÃ©es
â”œâ”€â”€ .gitignore                                 # Permet de ne pas afficher les Ã©lÃ©ments sÃ©lectionnÃ©s sur GitHub
â”œâ”€â”€ app.py                                     # Orchestre la vectorisation et la sauvegarde
â”œâ”€â”€ MistralChat.py                             # Script pour le lancement de l'API et de l'interface avec Streamlit
â”œâ”€â”€ poetry.lock                                # Pas versionnÃ© sur Git
â”œâ”€â”€ pyproject.toml                             # Gestion des dÃ©pendances Poetry
â”œâ”€â”€ README.md                                  # Documentation du projet

```
## Utilisation rapide
Proposition ici d'une installation rapide pour visionner l'API Rest et l'interface Streamlit.
Nous avons effectuÃ© beaucoup de changements entre le prototype et la nouvelle version alors dans le rapport technique nous irons en dÃ©tail dans le fonctionnement et les explications de ce que nous utilisons dans cette nouvelle proposition du chatbot.

### 1. Ajouter des documents

Placez vos documents dans le dossier `data/raw`.
Deux formats sont suportÃ©s pour le projet, il est possible de placer des documents en PDF ainsi que des fichiers excel.
- Les documents en PDF seront transformÃ©s et enregistrÃ©s dans une base vectorielle.
- Les fichiers excel seront nettoyÃ©s et ajoutÃ©s dans une base de donnÃ©es relationnelle (PostreSQL utilisÃ© ici).
- Pour maintenir une cohÃ©rence et une fiabilitÃ© dans nos donnÃ©es, les fichiers excel doivent respecter un certain format (vous pouvez par exemple celui utilisÃ©  dans data/raw).

### 2. GÃ©nÃ©ration des documents et de la base de donnÃ©es
#### Pour la crÃ©ation des vecteurs des documents en PDF
- Dans un premier temps assurez-vous d'avoir un dossier `vector_db/` dans le repo.
- Deux solutions s'offrent Ã  vous :

1. Lancer le script `scripts/build_index.py`
Cela va permettre la gÃ©nÃ©ration des vecteurs dans le dossier `vector_db/`
2. Lancer l'app.py
```
poetry run python app.py
```
Puis ouvrez un navigateur et se rendre sur la documentation swagger de notre API
```
http://localhost:7860/docs
```
Ici vous pouvez gÃ©nÃ©rer la base d'index via le bouton `rebuild_index`.
Cela va permettre Ã©galement la gÃ©nÃ©ration des vecteurs dans le dossier `vector_db/`

#### Pour la crÃ©ation de la base de donnÃ©es
L'enregistrement des datas dans la base donnÃ©es se fait dans une base PostreSQL en local.
1. Connexion Ã  une base PostreSQL
Choix de la BDD PostreSQL pour sa simplicitÃ© avec l'ORM SQLAlchemy.
CrÃ©ation d'une BDD en local :
- Ouvrez votre terminal puis lancez les commandes une Ã  une :
```
psql
CREATE DATABASE sportsee_nba_stats;
CREATE USER sportsee_user WITH PASSWORD '***';
GRANT ALL PRIVILEGES ON DATABASE sportsee_nba_stats TO sportsee_user;
ALTER DATABASE sportsee_nba_stats OWNER TO sportsee_user;
```
- AccÃ¨s Ã  la BDD
```
psql -U sportsee_user -d sportsee_nba_stats
```
2. Initialisation de la base de donnÃ©es
- Deux solutions s'offrent Ã  vous :
- Initialisation de la base de donnÃ©es avec `scripts/generation_db.py`
Lancement de ce script va importer vos donnÃ©es excel dans votre base.
- Initialisation de la base de donnÃ©es depuis l'API Rest :
Depuis la documentation Swagger vous pouvez gÃ©nÃ©rer la base de donnÃ©es via le bouton `rebuild_SQL_Base`.
- Les tables sont dÃ©sormais Ã  jour. Si vous avez installÃ© pgAdmin, vous pouvz siualsier facilement l'intÃ©gration des donnÃ©es.


## Rapport technique - du prototype au systÃ¨me actuel
### Reprise d'un prototype existant
Pour mener Ã  bien cette mission, nous avons eu Ã  disposition un prototype du chatbot. Dans un premier temps l'objectif a Ã©tÃ© de comprendre ce qui a Ã©tÃ© fait, quelle structure nous avons et ensuite de passer Ã  une Ã©valuation du systÃ¨me actuel via une Ã©valuation des mÃ©triques Ragas.
### Audit du prototype
1. **Organisation du projet**
La structure de l'ancien fichier Ã©tait la suivante :
```
â”œâ”€â”€ inputs/                   # Dossier contenant les donnÃ©es Ã  utiliser
â”‚   â”œâ”€â”€Reddit 1.pdf           # Capture d'Ã©cran de Reddit
â”‚   â”œâ”€â”€Reddit 2.pdf           # Capture d'Ã©cran de Reddit
â”‚   â”œâ”€â”€Reddit 3.pdf           # Capture d'Ã©cran de Reddit
â”‚   â”œâ”€â”€Reddit 4.pdf           # Capture d'Ã©cran de Reddit
â”‚   â”œâ”€â”€regular NBA.xlsx       # Fichier excel avec des statistiques NBA par joueur
â”œâ”€â”€ utils/                    # Scripts pour alimenter le fichier principal
â”‚   â”œâ”€â”€config.py              # Script contenant les configurations (le nom des paramÃ¨tres, des modÃ¨les etc)
â”‚   â”œâ”€â”€data_loader.py         # Script contenant le chargement des documents
â”‚   â”œâ”€â”€vector_store.py        # Script contenant les diffÃ©rentes fonctions allant de la crÃ©ation des dÃ©coupages Ã  l'enregistrement des vecteurs
â”œâ”€â”€ vector_db/                # Dossier avec l'enregistrement de notre base vectorielle
â”‚   â”œâ”€â”€document_chunks.pkl    # Les documents dÃ©coupÃ©s en format pkl
â”‚   â”œâ”€â”€faiss_index.idx        # La base d'index FAISS
â”œâ”€â”€ indexer.py                # Lancement de l'enregistrement de la base vectorielle
â”œâ”€â”€ MistralChat.py            # Lancement du chatbot sur une interface Streamlit
â”œâ”€â”€ README.md                 # Documentation du projet
â”œâ”€â”€ requirements.txt          # Fichier des dÃ©pendances
```

2. **Technologies utilisÃ©es**
- Language : Python
- Interface : Streamlit
- LLM & Embeddings MistralAI (mistral-small-latest /mistral-embed)
- Orchestration : Langchain
- Gestion des dÃ©pendances : fichier requirements

3. **Lancement de l'interface**
- Nous avons commencÃ© par lancer le projet afin de voir si il Ã©tait fonctionnel.
- On lance l'interface Streamlit. Sur votre terminal (bien vÃ©rifier que vous Ãªtes dans le bon dossier)
```
streamlit run MistralChat.py
```
- L'application fonctionne, on peut intÃ©ragir avec le chatbot et il propose des rÃ©ponses argumentÃ©es.
- Ã€ ce stade il est difficile d'Ã©valuer la cohÃ©rence et la pertinence des rÃ©ponses apportÃ©es par le chatbot.

4. **Analyse des performances du systÃ¨me**

L'entreprise nous a signalÃ© que les rÃ©ponses n'Ã©taient pas suffisantes pour eux. 
Afin de s'en rendre compte nous allons Ã©valuer le systÃ¨me avec les mÃ©triques de Ragas pour se faire notre propre avis.

- **GÃ©nÃ©ration des questions/rÃ©ponses**

L'objectif est d'Ã©valuer le modÃ¨le avec ragas, pour cela il faut avoir un jeu de questions/rÃ©ponses pour obtenir les mÃ©triques. 
CrÃ©ation du fichier **generation_answers.py** dans un nouveau dossier evaluations.

On y retouve 15 questions et 15 rÃ©ponses (humaines) portant sur le fichier excel avec plusieurs degrÃ©s de complexitÃ© :
- Questions faciles (valeurs directes)
- Questions intermÃ©diaires (comparaison simple)
- Questions plus difficiles (questions bruitÃ©es)
    
Ã€ la suite de ces questions, nous appelons notre systÃ¨me pour obtenir les rÃ©ponses du chatbot.

Dans le fichier csv gÃ©nÃ©rÃ© (dans le dossier resultat_evaluation.csv) nous retrouvons en plus des questions/rÃ©ponses (humaines + chatbot) :
- la liste des contextes utilisÃ©s par le chatbot pour fournir une rÃ©ponse (obligatoire pour ragas)
- le numÃ©ro des documents sÃ©lectionnÃ©s ainsi que son score de similaritÃ©
- **Lancement de l'Ã©valuation ragas**
Nous chargeons les mÃ©triques que nous voulons utiliser pour Ã©valuer le modÃ¨le (dans le fichier : first_ragas_evaluation.py) :
- **faithfulness** GÃ©nÃ©ration: fidÃ¨le au contexte ?
- **answer_relevancy** GÃ©nÃ©ration: rÃ©ponse pertinente Ã  la question ?
- **context_precision** RÃ©cupÃ©ration: contexte prÃ©cis (peu de bruit) ?
- **context_recall** RÃ©cupÃ©ration: infos clÃ©s rÃ©cupÃ©rÃ©es ?

Nous n'avons pas modifiÃ© le modÃ¨le afin d'Ã©valuer le prototype tel quel, nous avons juste ajoutÃ© une instruction au prompt pour demander au LLM de faire des rÃ©ponses courtes afin d'ajouter une certaine cohÃ©rence avec les rÃ©ponses que nous avons gÃ©nÃ©rÃ© de notre cÃ´tÃ©. Ensuite nous avons lancÃ© l'Ã©valuation.
Ce qu'il se passe :
- chaque question est analysÃ©e
- gÃ©nÃ©ration de 4 colonnes supplÃ©mentaires (les 4 mÃ©triques) dans le csv 
- Les scores sont entre 0 et 1, ce sont des scores normalisÃ©s, le 1 indique alors le meilleur score possible.

#### **RÃ©sultats de l'Ã©valuation sur l'ensemble des questions**
- Nous rÃ©cupÃ©rons notre csv et nous avons dÃ©cortiquÃ© les rÃ©sultats dans un notebook dÃ©diÃ©.
- Nous avons dÃ©jÃ  regardÃ© les scores moyens au global sur les 15 questions :

![alt text](notebooks/graph/Moyenne_metriques_ragas.png)

- Sur ce graphique nous avons dÃ©jÃ  de la maniÃ¨re pour une interprÃ©tation :

- On voit un score de "answer relevancy", pertinence de la rÃ©ponse, Ã©levÃ© en moyenne avec 0.91. Pour rappel lors du calcul de cette mÃ©trique, le LLM va gÃ©nÃ©rer des questions implicites Ã  partir de la rÃ©ponse, il va comparer les questions avec la question originale et le score est basÃ© sur la similaritÃ© sÃ©mantique.
    - Cela signifie que les rÃ©ponses sont bien alignÃ©es sÃ©mantiquement avec la question. Par contre une rÃ©ponse peut Ãªtre pertinente mais fausse.
- Le score de "faitfulness", la fidelitÃ© de la rÃ©ponse, est trÃ¨s bas avec 0.12 en moyenne sur les 15 questions. Cette mÃ©trique permet de dÃ©couper la rÃ©ponse gÃ©nÃ©rÃ©e en affirmation factuelle. Pour chaque affirmation, il y a une vÃ©rification qu'elles sont bien supportÃ©es par au moins un contexte. 
    - Le score atteste que les affirmations de la rÃ©ponse ne sont pas beaucoup appuyÃ©es sur le contexte gÃ©nÃ©rÃ©. Cela peut indiquer des hallucinations importantes.
- Le score de "context_precision", les documents rÃ©cupÃ©rÃ©s sont-ils utiles, est aussi bas avec 0.24 en moyenne. Pour chaque contexte,le LLM juge â€œCe contexte est-il nÃ©cessaire pour rÃ©pondre Ã  la question ?â€.
    - Un score de 0.24 signifie beaucoup de documents rÃ©cupÃ©rÃ©s et peu pertinents.Le systÃ¨me de retrieval ramÃ¨ne beaucoup de bruit.
- Le score de "context_recall", avons-nous rÃ©cupÃ©rÃ© toutes les infos nÃ©cessaires, est bas avec 0.21 en moyenne. Ici le LLM va identifier les informations clÃ©s requises pour rÃ©pondre Ã  la question. Ensuite il va vÃ©rifier si elles apparaissent dans le context.
    - 0.21 signifie qu'on ne rÃ©cupÃ¨re pas les bons documents ou on ne rÃ©cupÃ¨re quâ€™une petite partie des informations nÃ©cessaires.

- En conlusion de la moyenne globale :
- la relevancy Ã©levÃ©e montre que le LLM comprend bien la question.
- la faithfulness trÃ¨s basse, il invente ou extrapole.
- la precision basse, le retriever ramÃ¨ne du bruit.
- le recall bas, il manque des infos clÃ©s.

#### **RÃ©sultats de l'Ã©valuation par type de question**
- Regardons les rÃ©sultats par type de question :

![alt text](notebooks/graph/Moyenne_metriques_ragas_par_question.png)

On voit avec ce graphique que les scores globaux sont tirÃ©s vers le haut par les questions simples :
- Sur des questions factuelles, en posant des questions simples, courtes et prÃ©cises, le systÃ¨me s'en sort mieux qu'au global mais les scores restent trÃ¨s bas (hors answer relevancy). On devrait avoir des rÃ©sultats bein supÃ©reiurs sur ce type de question.
- Sur les questions intermÃ©diaires, c'est Ã  dire des questions un peu plus longues, des questions avec des comparaisons simples, les scores se dÃ©gradent pour toutes les mÃ©triques. On y voit nettement plus d'hallucinations et les rÃ©ponses ne s'appuyent pas sur le contexte mais de plus en plus sur des recherches internet via le LLM.
- Sur les questions bruitÃ©es, cela reste des questions avec des rÃ©ponses se trouvenat dans notre fichier excel mais elles sont volontairement moins explicites avec des formulations plus complexes, nous avons deux mÃ©triques Ã  0 (faithfulness et context_recall). Cela laisse paraÃ®tre une mauvaise rÃ©cupÃ©ration des documents.

- **Conclusion de cette premiÃ¨re Ã©valuation ragas**
En regardant uniquement les rÃ©ponses de l'interface du chatbot, il arrive Ã  rÃ©pondre Ã  toutes les questions mais en analysant les rÃ©ponses attendues et celles du chatbot ainsi que les rÃ©sultats des mÃ©triques, on identifie trÃ¨s vite les limites du modÃ¨le actuel.
Les scores dÃ©montrent un manque d'efficacitÃ© Ã  rÃ©cupÃ©rer les documents utiles pour apporter une rÃ©ponse cohÃ©rente et factuelle et va s'appuyer sur une recherche internet que par notre systÃ¨me RAG.

Nous avons alors regarder comment les documents sont gÃ©nÃ©rÃ©s et nous avons identifier ce qui pourrait Ãªtre le problÃ¨me. 
**Actuellement le modÃ¨le prend en compte le fichier excel comme un fichier texte.** En l'Ã©tat, le modÃ¨le prend en compte les donnÃ©es en texte et va les dÃ©couper, il va alors se "perdre" lors du retrieval et ne va pas Ãªtre capable de porposer des calculs si par exemple on lui demande de calculer le nombre de points d'une Ã©quipe en particulier.

- **DÃ©finition de notre nouveau objectif**
Une des Ã©tapes d'amÃ©lioration va Ãªtre de crÃ©er une base de donnÃ©es pour y dÃ©poser notre fichier de statistique, cela va permettre une meilleure organisation et permettre le calcul des donnÃ©es si besoin.
